{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette unité est divisée en deux sections :\n",
    "* Tout d'abord, nous verrons ce qui est nécessaire pour construire un système NLP capable de transformer un corps de texte en un tableau numérique de *caractéristiques*.\n",
    "* Ensuite, nous montrerons comment réaliser ces étapes à l'aide d'outils réels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construire un processeur de langage naturel à partir de zéro\n",
    "Dans cette section, nous utiliserons les bases de Python pour construire un système de traitement du langage naturel rudimentaire. Nous construirons un *corpus de documents* (deux petits fichiers texte), créerons un *vocabulaire* à partir de tous les mots contenus dans les deux documents, puis nous démontrerons une technique de *bag of words* pour extraire des caractéristiques de chaque document.<br>\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">**Cette première section n'est donnée qu'à titre d'illustration !**\n",
    "<br>Ne vous embêtez pas à mémoriser le code - nous ne ferions jamais cela dans la vie réelle.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commencez par quelques documents :\n",
    "Pour plus de simplicité, nous n'utiliserons pas de ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1.txt\n",
    "This is a story about cats\n",
    "our feline pets\n",
    "Cats are furry animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.txt\n",
    "This story is about surfing\n",
    "Catching waves is fun\n",
    "Surfing is a popular water sport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire un vocabulaire\n",
    "L'objectif est ici de construire un tableau numérique à partir de tous les mots qui apparaissent dans chaque document. Plus tard, nous créerons des instances (vecteurs) pour chaque document individuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 2, 'a': 3, 'story': 4, 'about': 5, 'cats': 6, 'our': 7, 'feline': 8, 'pets': 9, 'are': 10, 'furry': 11, 'animals': 12}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "i = 1\n",
    "\n",
    "with open('1.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "\n",
    "for word in x:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab[word]=i\n",
    "        i+=1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 2, 'a': 3, 'story': 4, 'about': 5, 'cats': 6, 'our': 7, 'feline': 8, 'pets': 9, 'are': 10, 'furry': 11, 'animals': 12, 'surfing': 13, 'catching': 14, 'waves': 15, 'fun': 16, 'popular': 17, 'water': 18, 'sport': 19}\n"
     ]
    }
   ],
   "source": [
    "with open('2.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "\n",
    "for word in x:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab[word]=i\n",
    "        i+=1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même si `2.txt` contient 15 mots, seuls 7 nouveaux mots ont été ajoutés au dictionnaire.\n",
    "\n",
    "## Extraction de caractéristiques\n",
    "Maintenant que nous avons encapsulé notre \"langue entière\" dans un dictionnaire, effectuons une *extraction de caractéristiques* sur chacun de nos documents originaux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty vector with space for each word in the vocabulary:\n",
    "one = ['1.txt']+[0]*len(vocab)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt', 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the frequencies of each word in 1.txt to our vector:\n",
    "with open('1.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "    \n",
    "for word in x:\n",
    "    one[vocab[word]]+=1\n",
    "    \n",
    "one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Nous pouvons constater que la plupart des mots de 1.txt n'apparaissent qu'une seule fois, bien que \"chats\" apparaisse deux fois.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the second document:\n",
    "two = ['2.txt']+[0]*len(vocab)\n",
    "\n",
    "with open('2.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "    \n",
    "for word in x:\n",
    "    two[vocab[word]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.txt', 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "['2.txt', 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Compare the two vectors:\n",
    "print(f'{one}\\n{two}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparant les vecteurs, nous constatons que certains mots sont communs aux deux, certains n'apparaissent que dans `1.txt`, d'autres que dans `2.txt`. En étendant cette logique à des dizaines de milliers de documents, nous verrions le dictionnaire de vocabulaire s'enrichir de centaines de milliers de mots. Les vecteurs contiendraient principalement des valeurs nulles, ce qui en ferait des *matrices éparses (sparse matrices)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words et Tf-idf\n",
    "Dans les exemples ci-dessus, chaque vecteur peut être considéré comme un *sac de mots (bag of words)*. En soi, ceux-ci peuvent ne pas être utiles jusqu'à ce que nous considérions les *fréquences de termes (term frequencies)*, c'est-à-dire la fréquence d'apparition des mots individuels dans les documents. Une manière simple de calculer la fréquence des termes consiste à diviser le nombre d'occurrences d'un mot par le nombre total de mots dans le document. De cette manière, le nombre d'occurrences d'un mot dans les documents volumineux peut être comparé à celui des documents plus petits.\n",
    "\n",
    "Cependant, il peut être difficile de différencier les documents sur la base de la fréquence des termes si un mot apparaît dans une majorité de documents. Dans ce cas, nous considérons également la *fréquence inverse du document (inverse document frequency)*, qui est le nombre total de documents divisé par le nombre de documents contenant le mot. En pratique, nous convertissons cette valeur sur une échelle logarithmique, comme suit [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency).\n",
    "\n",
    "\n",
    "L'ensemble de ces termes devient [**tf-idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words et Word Stems\n",
    "\n",
    "Certains mots comme \"the\" et \"and\" apparaissent si souvent, et dans tant de documents, qu'il n'est pas nécessaire de les compter. De même, il peut être judicieux de n'enregistrer que la racine d'un mot, par exemple `cat` au lieu de `cat` et `cats`. Cela réduira notre tableau de vocabulaire et améliorera les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization et Tagging\n",
    "Lorsque nous avons créé nos vecteurs, la première chose que nous avons faite a été de diviser le texte entrant sur les espaces blancs avec `.split()`. Il s'agissait d'une forme rudimentaire de *tokenisation*, c'est-à-dire de division d'un document en mots individuels. Dans cet exemple simple, nous ne nous sommes pas préoccupés de la ponctuation ou des différentes parties du discours. Dans le monde réel, nous nous appuyons sur une *morphologie* assez sophistiquée pour analyser le texte de manière appropriée.\n",
    "\n",
    "Une fois le texte divisé, nous pouvons revenir en arrière et \"étiqueter\" nos tokens avec des informations sur les parties du discours, les dépendances grammaticales, etc. Cela ajoute des dimensions supplémentaires à nos données et permet une compréhension plus approfondie du contexte de documents spécifiques. C'est pour cette raison que les vecteurs deviennent des matrices éparses ***high dimensional sparse matrices***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"margin: 20px\">**C'est la fin de la première section.**\n",
    "<br>Dans la section suivante, nous utiliserons scikit-learn pour effectuer une analyse réelle.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Extraction de caractéristiques à partir de textes\n",
    "Dans le notebook**Scikit-learn Primer**, nous avons appliqué un modèle de classification SVC simple à l'ensemble de données SMSSpamCollection. Nous avons essayé de prédire l'étiquette ham/spam en nous basant sur la longueur du message et le nombre de ponctuations. Dans cette section, nous allons examiner le texte de chaque message et tenter d'effectuer une classification basée sur le contenu. Nous tirerons parti de certaines des fonctionnalités de scikit-learn [feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) tools.\n",
    "\n",
    "## Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform imports and load the dataset:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./smsspamcollection.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérifier les valeurs manquantes :\n",
    "C'est toujours une bonne pratique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "length     0\n",
       "punct      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jetez un coup d'œil rapide à la colonne `label` *ham* et *spam* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>4825 messages sur 5572, soit 86,6 %, sont des messages de jambon. Cela signifie que tout modèle de classification de texte que nous créons doit être **plus performant que 86,6 %** pour battre le hasard.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diviser les données en ensembles d'entrainement et de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['message']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn's CountVectorizer\n",
    "\n",
    "Le prétraitement du texte, la tokenisation et la possibilité de filtrer les stop words sont tous inclus dans [CountVectorizer] (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), qui construit un dictionnaire de caractéristiques et transforme les documents en vecteurs de caractéristiques (feature vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Cela montre que notre ensemble d'entrainement est composé de 3733 documents et de 7082 caractéristiques.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer les nombres en fréquences avec Tf-idf\n",
    "Bien que le comptage des mots soit utile, les documents plus longs auront des valeurs moyennes de comptage plus élevées que les documents plus courts, même s'ils traitent des mêmes sujets.\n",
    "\n",
    "Pour éviter cela, nous pouvons simplement diviser le nombre d'occurrences de chaque mot dans un document par le nombre total de mots dans le document : ces nouvelles caractéristiques sont appelées **tf** pour Term Frequencies.\n",
    "\n",
    "Un autre raffinement de **tf** consiste à réduire les poids des mots qui apparaissent dans de nombreux documents du corpus et qui sont donc moins informatifs que ceux qui n'apparaissent que dans une plus petite partie du corpus.\n",
    "\n",
    "Cette réduction d'échelle est appelée **tf-idf** pour \"Term Frequency times Inverse Document Frequency\" (fréquence des termes multipliée par la fréquence inverse des documents).\n",
    "\n",
    "Le tf et le tf-idf peuvent être calculés comme suit en utilisant [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : la méthode `fit_transform()` effectue en fait deux opérations : elle ajuste un estimateur aux données et transforme ensuite notre matrice de comptage en une représentation tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combiner les étapes avec TfidVectorizer\n",
    "À l'avenir, nous pourrons combiner les étapes CountVectorizer et TfidTransformer en une seule à l'aide de la fonction [TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Former un classificateur\n",
    "Nous présentons ici un classificateur SVM similaire au SVC, appelé [LinearSVC] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). LinearSVC gère mieux les données éparses et s'adapte bien à un grand nombre d'échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Plus tôt, nous avons nommé notre classifieur SVC **svc_model**. Ici, nous utilisons le nom plus générique **clf** (pour classifier).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire un pipeline\n",
    "Rappelez-vous que seul notre ensemble d'apprentissage a été vectorisé en un vocabulaire complet. Afin d'effectuer une analyse sur notre jeu de test, nous devrons le soumettre aux mêmes procédures. Heureusement, scikit-learn propose une classe [**Pipeline**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) qui se comporte comme un classificateur composé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aluboya\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py:209: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(joblib_version) < '0.12':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester le classifieur et afficher les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1586    7]\n",
      " [  12  234]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1593\n",
      "        spam       0.97      0.95      0.96       246\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1839\n",
      "   macro avg       0.98      0.97      0.98      1839\n",
      "weighted avg       0.99      0.99      0.99      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989668297988037\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant le texte des messages, notre modèle a obtenu d'excellents résultats : il a prédit correctement le spam dans 98,97 % des cas !\n",
    "Appliquons maintenant ce que nous avons appris à un projet de classification de texte impliquant des critiques de films positives et négatives.\n",
    "\n",
    "## Suivant : Projet de classification de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
