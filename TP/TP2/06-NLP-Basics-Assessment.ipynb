{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ère partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Créer un objet Doc à partir du fichier `owlcreek.txt`**<br>\n",
    "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "\n",
    "with open('../../data/TextFiles/owlcreek.txt', encoding='unicode-escape') as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "I\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to verify it worked:\n",
    "\n",
    "doc[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Combien de tokens sont contenus dans le fichier ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Combien de phrases sont contenues dans le fichier ?<br>HINT: Vous devez d'abord créer une liste !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sent for sent in doc.sents]\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Imprimer la deuxième phrase du document**<br> HINT: L'indexation commence à zéro et le titre compte comme la première phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The man's hands were behind\n",
       "his back, the wrists bound with a cord.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Pour chaque élément de la phrase ci-dessus, imprimez son`text`, `POS` tag, `dep` tag and `lemma`<br>\n",
    "CHALLENGE: Faire en sorte que les valeurs s'alignent en colonnes dans la sortie imprimée.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The             DET        det        the       \n",
      "man             NOUN       poss       man       \n",
      "'s              PART       case       's        \n",
      "hands           NOUN       nsubj      hand      \n",
      "were            AUX        ROOT       be        \n",
      "behind          ADP        prep       behind    \n",
      "\n",
      "               SPACE      dep        \n",
      "         \n",
      "his             PRON       poss       his       \n",
      "back            NOUN       pobj       back      \n",
      ",               PUNCT      punct      ,         \n",
      "the             DET        det        the       \n",
      "wrists          NOUN       appos      wrist     \n",
      "bound           VERB       acl        bind      \n",
      "with            ADP        prep       with      \n",
      "a               DET        det        a         \n",
      "cord            NOUN       pobj       cord      \n",
      ".               PUNCT      punct      .         \n",
      "                SPACE      dep                  \n"
     ]
    }
   ],
   "source": [
    "for token in sentences[1] :\n",
    "    print(f'{token.text:{15}} {token.pos_:{10}} {token.dep_:{10}} {token.lemma_:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ème partie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1: Tokenisation de phrases avec SpaCy\n",
    "\n",
    "Question : Créez une phrase complexe en anglais et utilisez SpaCy pour la tokeniser en phrases individuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"My name is Guillaume and I'm 23 years old, I live in Neuilly-Plaisance and I like to play tennis.\"\n",
    "\n",
    "doc = nlp(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Utilisez SpaCy pour tokeniser la phrase complexe en mots.\n",
    "\n",
    "Question : Quels avantages offre SpaCy par rapport à d'autres bibliothèques pour la tokenisation?\n",
    "\n",
    "Question : Créez une fonction custom_spacy_tokenizer qui prend une phrase en entrée, utilise SpaCy pour la tokeniser et renvoie les tokens en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_tokenizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "name\n",
      "is\n",
      "Guillaume\n",
      "and\n",
      "I\n",
      "'m\n",
      "23\n",
      "years\n",
      "old\n",
      ",\n",
      "I\n",
      "live\n",
      "in\n",
      "Neuilly\n",
      "-\n",
      "Plaisance\n",
      "and\n",
      "I\n",
      "like\n",
      "to\n",
      "play\n",
      "tennis\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy offre une excellente performance et une précision extrêmement élevées pour le traitement du langage naturel, y compris pour de grandes quantités de texte.\n",
    "- facile a prendre en main\n",
    "- il prend en charge 64 langues différentes\n",
    "- fonctionnalites avancées telles que la détection des entités nommées, l'analyse grammaticale, et d'autres tâches de traitement du langage naturel.\n",
    "- personnaliser ses modeles pre-entrainés pour des taches spécifiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_spacy_tokenizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'guillaume',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " '23',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'i',\n",
       " 'live',\n",
       " 'in',\n",
       " 'neuilly',\n",
       " '-',\n",
       " 'plaisance',\n",
       " 'and',\n",
       " 'i',\n",
       " 'like',\n",
       " 'to',\n",
       " 'play',\n",
       " 'tennis',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_spacy_tokenizer(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3\n",
    "\n",
    "Question : Créez une fonction custom_spacy_tokenizer qui prend une phrase en entrée, utilise SpaCy pour la tokeniser et renvoie les tokens en minuscules.\n",
    "\n",
    "Question : Comment SpaCy gère-t-il la tokenisation des entités nommées? Testez cela sur une phrase contenant une entité nommée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'is', 'a', 'major', 'technology', 'company', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenisation des entités nommées avec SpaCy\n",
    "text_with_entity = \"Apple is a major technology company.\"\n",
    "\n",
    "def custom_spacy_tokenizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text.lower() for token in doc]\n",
    "\n",
    "custom_spacy_tokenizer(text_with_entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SpaCy utilise des modèles linguistiques pré-entraînés pour reconnaître et tokeniser les entités nommées, assurant une représentation cohérente des entités dans le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer le stemming sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming --> stem\n",
      "is --> is\n",
      "an --> an\n",
      "essential --> essenti\n",
      "part --> part\n",
      "of --> of\n",
      "natural --> natur\n",
      "language --> languag\n",
      "processing. --> processing.\n",
      "It --> It\n",
      "involves --> involv\n",
      "reducing --> reduc\n",
      "words --> word\n",
      "to --> to\n",
      "their --> their\n",
      "base --> base\n",
      "form. --> form.\n",
      "running --> run\n"
     ]
    }
   ],
   "source": [
    "# Création d'une phrase complexe\n",
    "text = \"Stemming is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in text.split():\n",
    "    print(word+' --> '+ stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_stemmer qui prend une phrase en entrée, utilise SpaCy pour effectuer le stemming, et renvoie les stems en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_stemmer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem',\n",
       " 'is',\n",
       " 'an',\n",
       " 'essenti',\n",
       " 'part',\n",
       " 'of',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'processing.',\n",
       " 'it',\n",
       " 'involv',\n",
       " 'reduc',\n",
       " 'word',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'form.',\n",
       " 'run']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de stemming personnalisée avec SpaCy\n",
    "\n",
    "def custom_spacy_stemmer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [stemmer.stem(word).lower() for word in text.split()]\n",
    "\n",
    "custom_spacy_stemmer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1 \n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer la lemmatisation sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization   NOUN       lemmatization\n",
      "is              AUX        be        \n",
      "an              DET        an        \n",
      "essential       ADJ        essential \n",
      "part            NOUN       part      \n",
      "of              ADP        of        \n",
      "natural         ADJ        natural   \n",
      "language        NOUN       language  \n",
      "processing      NOUN       processing\n",
      ".               PUNCT      .         \n",
      "It              PRON       it        \n",
      "involves        VERB       involve   \n",
      "reducing        VERB       reduce    \n",
      "words           NOUN       word      \n",
      "to              ADP        to        \n",
      "their           PRON       their     \n",
      "base            NOUN       base      \n",
      "form            NOUN       form      \n",
      ".               PUNCT      .         \n"
     ]
    }
   ],
   "source": [
    "# Création d'une phrase complexe\n",
    "text = \"Lemmatization is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{15}} {token.pos_:{10}} {token.lemma_:{10}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_lemmatizer qui prend une phrase en entrée, utilise SpaCy pour effectuer la lemmatisation, et renvoie les lemmes en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_lemmatizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lemmatization',\n",
       " 'be',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'part',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'it',\n",
       " 'involve',\n",
       " 'reduce',\n",
       " 'word',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'form',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de lemmatisation personnalisée avec SpaCy\n",
    "\n",
    "def custom_spacy_lemmatizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.lemma_.lower() for token in doc]\n",
    "\n",
    "custom_spacy_lemmatizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour filtrer les stopwords de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'during', 'text', 'preprocessing', '.']\n",
      "Filtered words (without stopwords): ['Stopwords', 'common', 'words', 'removed', 'text', 'preprocessing', '.']\n",
      "SpaCy Stopwords: {'then', 'twenty', 'using', \"'s\", 'meanwhile', 'done', 'whether', 'ourselves', 'have', 'when', 'than', 'at', 'had', 'both', 'whom', 'empty', 'those', 'do', 'what', 'afterwards', 'too', '‘d', 'due', '’re', 'now', 'thereby', 'below', 'either', 'twelve', '‘m', 'does', 'whereafter', 'itself', \"'m\", 'nowhere', 'against', '’ll', 'everything', 'never', 'them', 'whenever', 'everyone', 'many', 'whence', 'would', 'ten', 'elsewhere', 'eight', 'ever', 'five', 'has', 'already', 'he', 'yourselves', 'thru', 'since', 'six', 'least', 'keep', 'wherein', 'thereafter', 'beside', 'go', 'whole', 'upon', 'been', 'full', 'may', 'everywhere', 'be', 'noone', 'hence', 'cannot', 'four', 'because', 'or', 'often', \"'ll\", 'until', 'within', 'else', 'something', 'say', 'and', 'enough', 'along', 'although', '‘ll', 'besides', 'across', 'anyone', 'indeed', 'himself', 're', 'fifty', 'seemed', 'wherever', 'my', 'therein', 'their', 'some', 'being', 'a', 'latter', 'we', 'less', 'to', 'out', 'sometime', 'sixty', 'yourself', 'no', 'that', 'for', 'put', 'former', 'might', 'take', 'just', 'except', 'without', 'mine', 'forty', 'how', 'your', 'via', 'whatever', 'though', 'among', 'call', 'front', 'nine', 'once', 'whither', 'anyway', 'down', 'onto', 'therefore', 'again', 'still', 'she', 'in', 'used', 'very', 'much', 'towards', 'sometimes', 'fifteen', 'moreover', 'seeming', 'should', 'another', 'through', 'none', 'him', 'made', 'whereby', 'did', 'are', 'always', 'must', 'last', 'make', 'with', 'get', 'above', 'not', 'as', 'doing', 'after', 'nor', 'amount', 'each', 'hereupon', '’s', 'thence', 'about', 'back', 'us', \"n't\", 'alone', 'thus', 'from', 'seem', 'am', 'between', 'whereupon', 'other', 'amongst', 'off', \"'d\", 'perhaps', 'anywhere', 'becoming', 'first', 'hundred', 'before', 'bottom', 'thereupon', 'various', 'became', 'herself', 'per', \"'ve\", 'top', 'was', 'this', 'myself', 'two', 'yours', 'almost', '’m', 'however', 'regarding', 'quite', 'up', 'all', 'they', 'unless', 'whoever', 'anyhow', 'you', 'give', 'latterly', '’ve', 'most', 'become', 'n‘t', 'during', 'yet', 'same', 'otherwise', 'even', 'well', 'serious', 'somewhere', 'next', 'hereby', 'its', 'ours', '‘re', 'others', 'of', 'mostly', 'herein', 'under', 'her', 'neither', 'few', 'whereas', 'such', 'will', 'really', 'themselves', 'one', 'while', 'his', 'by', 'over', 'formerly', 'also', '‘s', 'behind', 'three', 'name', 'anything', 'i', 'who', 'me', 'beyond', 'can', 'the', 'nevertheless', 'move', 'further', 'is', \"'re\", 'someone', 'into', 'n’t', 'any', 'our', 'third', 'if', 'were', 'only', 'every', 'nothing', 'see', 'show', 'hereafter', 'toward', 'could', 'part', 'throughout', 'why', 'whose', 'more', '’d', 'but', 'where', 'nobody', 'together', 'here', 'somehow', 'eleven', 'around', 'own', 'rather', 'so', 'becomes', 'hers', 'it', 'there', 'on', 'side', '‘ve', 'seems', 'namely', 'please', 'which', 'several', 'ca', 'these', 'an', 'beforehand'}\n"
     ]
    }
   ],
   "source": [
    "text = \"Stopwords are common words that are often removed during text preprocessing.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "original_words = [token.text for token in doc]\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(\"Original words:\", original_words)\n",
    "print(\"Filtered words (without stopwords):\", filtered_words)\n",
    "print(\"SpaCy Stopwords:\", nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction remove_stopwords qui prend une phrase en entrée, utilise SpaCy pour filtrer les stopwords, et renvoie les mots restants en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction remove_stopwords à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stopwords', 'common', 'words', 'removed', 'text', 'preprocessing', '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de filtrage des stopwords avec SpaCy\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codez une fonction 'preprocess_text_spacy' qui applique les méthodes de preprocessing vues en cours et permet d'obtenir les résultats prétraités suivants :\n",
    "\n",
    "NB : n'oublier pas de traiter les caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_spacy(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    preprocessed_text = [re.sub(r'[^a-zA-Z]', '', token.text.lower()) for token in doc if not token.is_stop]\n",
    "\n",
    "    return ' '.join(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello  know natural language processing fascinating field   '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple d'utilisation avec SpaCy\n",
    "raw_text = \"Hello, I know that Natural Language Processing is a fascinating field!!!\"\n",
    "\n",
    "preprocess_text_spacy(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
