{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Souvent, lorsque l'on recherche un mot clé dans un texte, il est utile que la recherche renvoie des variantes du mot. Par exemple, la recherche de \"boat\" peut également renvoyer \"boats\" et \"boating\". Dans ce cas, \"boat\" serait le **symbole** de [boat, boater, boating, boats].\n",
    "\n",
    "Le stemming est une méthode quelque peu rudimentaire pour cataloguer les mots apparentés ; elle consiste essentiellement à supprimer des lettres à partir de la fin du mot jusqu'à ce que l'on atteigne le stem. Cette méthode fonctionne assez bien dans la plupart des cas, mais l'anglais présente malheureusement de nombreuses exceptions nécessitant un processus plus sophistiqué. En fait, spaCy n'inclut pas de stemmer, choisissant plutôt de s'appuyer entièrement sur la lemmatisation. Pour les personnes intéressées, il y a quelques informations sur cette décision [ici] (https://github.com/explosion/spaCy/issues/327). Nous discuterons des vertus de la *lemmatisation* dans la section suivante.\n",
    "\n",
    "À la place, nous utiliserons un autre outil de TAL populaire appelé **nltk**, qui signifie *Natural Language Toolkit* (boîte à outils pour le langage naturel). Pour plus d'informations sur nltk, visitez le site https://www.nltk.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "L'un des outils de stemming les plus courants - et les plus efficaces - est l'[*algorithme de Porter*] (https://tartarus.org/martin/PorterStemmer/) développé par Martin Porter en [1980] (https://tartarus.org/martin/PorterStemmer/def.txt). L'algorithme utilise cinq phases de réduction des mots, chacune avec son propre ensemble de règles de mise en correspondance. Dans la première phase, des règles simples de mise en correspondance des suffixes sont définies, telles que :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stemming1.png](../stemming1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir d'un ensemble donné de règles de troncature, une seule règle est appliquée, basée sur le suffixe S1 le plus long. Ainsi, `caresses` se réduit à `caress` mais pas à `cares`.\n",
    "\n",
    "Des phases plus sophistiquées prennent en compte la longueur/complexité du mot avant d'appliquer une règle. Par exemple, les phases plus sophistiquées prennent en compte la longueur/complexité du mot avant d'appliquer une règle :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stemming1.png](../stemming2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, `m>0` décrit la \"mesure\" de la tige, de sorte que la règle s'applique à toutes les tiges sauf les plus élémentaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aluboya\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Notez que la racine reconnaît \"runner\" comme un nom, et non comme une forme verbale ou un participe. De même, les adverbes \"easy\" et \"fairly\" sont dérivés de la racine inhabituelle \"easili\" et \"fairli\".\"</font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "Il s'agit d'une appellation quelque peu erronée, car Snowball est le nom d'un langage de troncature développé par Martin Porter. L'algorithme utilisé ici est plus précisément appelé \"English Stemmer\" ou \"Porter2 Stemmer\". Il offre une légère amélioration par rapport à l'algorithme original de Porter, à la fois en termes de logique et de rapidité. Comme **nltk** utilise le nom SnowballStemmer, nous l'utiliserons ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "# words = ['generous','generation','generously','generate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=green>Dans ce cas, la performance du stemmer est la même que celle du Porter Stemmer, à l'exception du fait qu'il a traité le stem de \"fairly\" de manière plus appropriée avec \"fair\"</font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essayez vous-même !\n",
    "#### Introduisez quelques-uns de vos propres mots et testez les différents logiciels de troncature sur ces mots. N'oubliez pas de les passer en tant que chaînes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['consolingly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "consolingly --> consolingli\n"
     ]
    }
   ],
   "source": [
    "print('Porter Stemmer:')\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter2 Stemmer:\n",
      "consolingly --> consol\n"
     ]
    }
   ],
   "source": [
    "print('Porter2 Stemmer:')\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le stemming a ses inconvénients. Si l'on donne le token `saw`, le stemming pourrait toujours retourner `saw`, alors que la lemmatisation retournerait probablement soit `see` soit `saw` selon que l'utilisation du token est un verbe ou un substantif. Prenons l'exemple suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> I\n",
      "am --> am\n",
      "meeting --> meet\n",
      "him --> him\n",
      "tomorrow --> tomorrow\n",
      "at --> at\n",
      "the --> the\n",
      "meeting --> meet\n"
     ]
    }
   ],
   "source": [
    "phrase = 'I am meeting him tomorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, le mot \"réunion\" apparaît deux fois - une fois en tant que verbe et une fois en tant que nom, et pourtant le radical traite les deux de la même manière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next : Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
